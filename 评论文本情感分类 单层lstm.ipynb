{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn,rnn_cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型 双向LSTM + 评论特征向量attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HUAPA_ONE_LAYER(object):\n",
    "    def __init__(self,max_len,class_num,embedding_file,embedding_dim,hidden_size,feature_dim,attention_dim,train,regularizer):\n",
    "        #每个评论中包含的单词数\n",
    "        self.max_len = max_len\n",
    "        #评论类别\n",
    "        self.class_num = class_num\n",
    "        #词嵌入文件\n",
    "        self.embedding_file = embedding_file\n",
    "        #词嵌入维度\n",
    "        self.embedding_dim = embedding_dim\n",
    "        #隐藏状态维度\n",
    "        self.hidden_size = hidden_size\n",
    "        #简单向量维度\n",
    "        self.feature_dim = feature_dim\n",
    "        #attention层中w的维度\n",
    "        self.attention_dim = attention_dim\n",
    "        #是否训练过程\n",
    "        self.train = train\n",
    "        #是否正则化\n",
    "        self.regularizer= regularizer\n",
    "        \n",
    "        with tf.name_scope('input'):\n",
    "            #评论文本特征的嵌入向量\n",
    "            self.doc_feature = tf.placeholder(tf.float32,[None,self.feature_dim],name='doc_feature')\n",
    "            #input_x的维度[评论数量，评论中包含的单词数]\n",
    "            self.input_x = tf.placeholder(tf.int32,[None,self.max_len],name='input_x')\n",
    "            #input_y的维度[评论数量，评论的标签数目]  one-hot编码\n",
    "            self.input_y = tf.placeholder(tf.float32,[None,self.class_num],name='input_y')\n",
    "            #评论的长度(即单词数)\n",
    "            self.doc_len = tf.placeholder(tf.int32,[None],name='doc_len')\n",
    "            \n",
    "        with tf.name_scope('weights'):\n",
    "            self.weights = {\n",
    "                #决策的softmax层\n",
    "                'softmax' : tf.Variable(tf.random_uniform([2*hidden_size,self.class_num],-0.01,0.01)),\n",
    "                #attention层参数\n",
    "                'wh' : tf.Variable(tf.random_uniform([2*hidden_size,self.attention_dim],-0.01,0.01)),\n",
    "                'v' : tf.Variable(tf.random_uniform([self.attention_dim,1],-0.01,0.01)),\n",
    "                #评论特征嵌入参数\n",
    "                'wf' : tf.Variable(tf.random_uniform([self.feature_dim,self.attention_dim],-0.01,0.01))\n",
    "            }\n",
    "            \n",
    "        with tf.name_scope('biases'):\n",
    "            self.biases = {\n",
    "                'softmax' : tf.Variable(tf.random_uniform([self.class_num],-0.01,0.01)),\n",
    "                'wh' : tf.Variable(tf.random_uniform([self.attention_dim],-0.01,0.01))\n",
    "            }\n",
    "            \n",
    "        with tf.name_scope('embedding'):\n",
    "            #词嵌入文本，不做训练\n",
    "            self.word_embedding = tf.constant(self.embedding_file,name='word_embedding',dtype=tf.float32)\n",
    "            #x的维度为[输入数据量，评论中包含的sentence数量，sentence中包含的词汇数量,word嵌入维度]\n",
    "            self.x = tf.nn.embedding_lookup(self.word_embedding,self.input_x)\n",
    "    \n",
    "    def softmax(self, inputs, length, max_length):\n",
    "        inputs = tf.cast(inputs, tf.float32)\n",
    "        inputs = tf.exp(inputs)\n",
    "        length = tf.reshape(length, [-1])\n",
    "        mask = tf.reshape(tf.cast(tf.sequence_mask(length, max_length), tf.float32), tf.shape(inputs))\n",
    "        inputs *= mask\n",
    "        _sum = tf.reduce_sum(inputs, reduction_indices=2, keep_dims=True) + 1e-9\n",
    "        return inputs / _sum  \n",
    "    \n",
    "    def feature_attention(self):\n",
    "        #inputs 的维度是 [评论数量，词汇个数（短语长度）,单词嵌入维度] \n",
    "        inputs = tf.reshape(self.x, [-1, self.max_len, self.embedding_dim])\n",
    "        #LSTM层\n",
    "        with tf.name_scope('word_encode'):\n",
    "            outputs, state = tf.nn.bidirectional_dynamic_rnn(\n",
    "                cell_fw=tf.nn.rnn_cell.LSTMCell(self.hidden_size, forget_bias=1.0),\n",
    "                cell_bw=tf.nn.rnn_cell.LSTMCell(self.hidden_size, forget_bias=1.0),\n",
    "                inputs=inputs,\n",
    "                sequence_length=self.doc_len,\n",
    "                dtype=tf.float32,\n",
    "                scope='word'\n",
    "            )\n",
    "            #outputs的维度应该是[评论数量，词汇数，隐状态维度*2]\n",
    "            outputs = tf.concat(outputs,2)\n",
    "        \n",
    "        #attention层\n",
    "        #输入数量\n",
    "        batch_size = tf.shape(outputs)[0]\n",
    "        with tf.name_scope('word_attention'):\n",
    "            #output维度 [评论数量，隐藏状态]\n",
    "            output = tf.reshape(outputs, [-1, 2 * self.hidden_size])\n",
    "            u = tf.matmul(output, self.weights['wh']) + self.biases['wh']\n",
    "            #u的维度 [评论数量，最大词汇数，隐藏状态]\n",
    "            u = tf.reshape(u, [-1, self.max_len, self.attention_dim])\n",
    "            u += tf.matmul(self.doc_feature, self.weights['wf'])[:,None,:]\n",
    "            u = tf.tanh(u)\n",
    "            u = tf.reshape(u, [-1, self.attention_dim])\n",
    "            #alpha的维度是[评论数，1,最大词汇数]\n",
    "            alpha = tf.reshape(tf.matmul(u, self.weights['v']),\n",
    "                               [batch_size, 1, self.max_len])\n",
    "            alpha = self.softmax(alpha, self.doc_len, self.max_len)\n",
    "            outputs = tf.matmul(alpha, outputs)\n",
    "            \n",
    "        with tf.name_scope('softmax'):\n",
    "            self.doc = tf.reshape(outputs, [batch_size, 2 * self.hidden_size],name='doc_vectors')\n",
    "            self.scores = tf.matmul(self.doc, self.weights['softmax']) + self.biases['softmax']\n",
    "            #添加正则\n",
    "            #if self.train:\n",
    "            #if self.regularizer != None:\n",
    "            tf.add_to_collection('losses',self.regularizer(self.weights['softmax']))\n",
    "            #添加dropout层\n",
    "            #    self.scores = tf.nn.dropout(self.scores,0.5)\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.scores, labels=tf.argmax(self.input_y,1))\n",
    "            self.loss = tf.reduce_mean(losses) + tf.add_n(tf.get_collection('losses'))\n",
    "\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.correct_num = tf.reduce_sum(tf.cast(correct_predictions, dtype=tf.int32),name='correct_num')\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "            \n",
    "        with tf.name_scope('metrics'):\n",
    "            self.mse = tf.reduce_sum(tf.square(self.predictions - tf.argmax(self.input_y, 1)),name='mse')\n",
    "\n",
    "    def build_model(self):\n",
    "        self.feature_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 工具类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 词嵌入向量\n",
    "def load_embedding(embedding_file_path, corpus, embedding_dim):\n",
    "    wordset = set();\n",
    "    for line in corpus:\n",
    "        line = line.strip().split()\n",
    "        for w in line:\n",
    "            wordset.add(w.lower())\n",
    "    words_dict = dict(); word_embedding = []; index = 1\n",
    "    words_dict['$EOF$'] = 0  #add EOF\n",
    "    word_embedding.append(np.zeros(embedding_dim))\n",
    "    with open(embedding_file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            check = line.strip().split()\n",
    "            if len(check) > embedding_dim + 1 or len(check) == 2:\n",
    "                continue\n",
    "            line = line.strip().split()\n",
    "            if line[0] not in wordset: \n",
    "                continue\n",
    "            embedding = [float(s) for s in line[1:]]\n",
    "            word_embedding.append(embedding)\n",
    "            words_dict[line[0]] = index\n",
    "            index +=1\n",
    "    return np.asarray(word_embedding), words_dict\n",
    "\n",
    "#将文本转化为词索引表示\n",
    "def fit_transform(x_text,word_dict,max_len):\n",
    "    x, doc_len = [], []\n",
    "    for index,doc in enumerate(x_text):\n",
    "        t_x = np.zeros((max_len),dtype=int)\n",
    "        i = 0\n",
    "        for word in doc.strip().split():\n",
    "            if i >= max_len:\n",
    "                break\n",
    "            if word not in word_dict:\n",
    "                continue\n",
    "            t_x[i] = word_dict[word]\n",
    "            i += 1\n",
    "        #记录评论中词汇长度\n",
    "        x.append(t_x)\n",
    "        doc_len.append(i)\n",
    "    return np.asarray(x),np.asarray(doc_len)\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self,data_file,feature_file):\n",
    "        self.t_feature = []\n",
    "        self.t_label = []\n",
    "        self.t_docs = []\n",
    "        with open(data_file,'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip().split('\\t')\n",
    "                self.t_label.append(int(line[7])+1)\n",
    "                self.t_docs.append(line[4].lower())\n",
    "        with open(feature_file,'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip().split('\\t')\n",
    "                self.t_feature.append(np.asarray(line[1][1:-1].split(',')))\n",
    "            self.data_size = len(self.t_docs)\n",
    "    \n",
    "    #全部数据转换，用来生成文档向量表示\n",
    "    def getAllData(self,wordsdict,max_len,n_class):\n",
    "        self.all_labels = np.eye(n_class,dtype=np.float32)[self.t_label]\n",
    "        self.all_docs ,self.all_docs_len = fit_transform(self.t_docs,wordsdict,max_len)\n",
    "        self.all_features = self.t_feature\n",
    "    \n",
    "    def genBatch(self,wordsdict,batch_size,max_len,n_class):\n",
    "        self.epoch = (int)(len(self.t_docs) / batch_size)\n",
    "        if len(self.t_docs) % batch_size !=0:\n",
    "            self.epoch += 1\n",
    "        self.labels = []\n",
    "        self.docs = []\n",
    "        self.doc_len = []\n",
    "        self.features = []\n",
    "        \n",
    "        for i in range(self.epoch):\n",
    "            self.features.append(np.asarray(self.t_feature[i*batch_size:(i+1)*batch_size]))\n",
    "            self.labels.append(np.eye(n_class,dtype=np.float32)[self.t_label[i*batch_size:(i+1)*batch_size]])\n",
    "            b_docs,b_doc_lens = fit_transform(self.t_docs[i*batch_size:(i+1)*batch_size],wordsdict,max_len)\n",
    "            self.docs.append(b_docs)\n",
    "            self.doc_len.append(b_doc_lens)\n",
    "    \n",
    "    def batch_iter(self,wordsdict,n_class,batch_size,num_epochs,max_len,shuffle=True):\n",
    "        data_size = len(self.t_docs)\n",
    "        num_batches_per_epoch = int(data_size / batch_size) + \\\n",
    "                                (1 if data_size % batch_size else 0)\n",
    "        self.t_label = np.asarray(self.t_label)\n",
    "        self.t_docs = np.asarray(self.t_docs)\n",
    "        self.t_feature = np.asarray(self.t_feature)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            if shuffle:\n",
    "                shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "                self.t_label = self.t_label[shuffle_indices]\n",
    "                self.t_docs = self.t_docs[shuffle_indices]\n",
    "                self.t_feature = self.t_feature[shuffle_indices]\n",
    "            \n",
    "            for batch_num in range(num_batches_per_epoch):\n",
    "                start = batch_num * batch_size\n",
    "                end = min((batch_num + 1) * batch_size, data_size)\n",
    "                label = np.eye(n_class, dtype=np.float32)[self.t_label[start:end]]\n",
    "                features = self.t_feature[start:end]\n",
    "                b_docs,b_doc_lens = fit_transform(self.t_docs[start:end],wordsdict,max_len)\n",
    "                batch_data = zip(features, b_docs, label, b_doc_lens)\n",
    "                yield batch_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 划分训练集,验证集，测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#总共数据量\n",
    "n = 15433\n",
    "#训练集60% 验证集20% 测试集20%\n",
    "n_train = (int)(n * 0.6)\n",
    "n_validate = n_train + (int)(n * 0.2)\n",
    "with open('./Video_text.txt') as f:\n",
    "    l = f.readlines()\n",
    "    with open('./Video_text_train.txt','w') as a:\n",
    "        a.writelines(l[0:n_train])\n",
    "    with open('./Video_text_validate.txt','w') as b:\n",
    "        b.writelines(l[n_train:n_validate])\n",
    "    with open('./Video_text_test.txt','w') as c:\n",
    "        c.writelines(l[n_validate:])\n",
    "with open('./Video_simple_feature.txt') as f:\n",
    "    l = f.readlines()\n",
    "    with open('./Video_simple_feature_train.txt','w') as a:\n",
    "        a.writelines(l[0:n_train])\n",
    "    with open('./Video_simple_feature_validate.txt','w') as b:\n",
    "        b.writelines(l[n_train:n_validate])\n",
    "    with open('./Video_simple_feature_test.txt','w') as c:\n",
    "        c.writelines(l[n_validate:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,datetime,time, pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading params\n",
    "tf.flags.DEFINE_integer(\"n_class\", 3, \"Numbers of class\")\n",
    "\n",
    "#Model hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 200, \"Dimensionality of character embedding\")\n",
    "tf.flags.DEFINE_integer(\"hidden_size\", 50, \"hidden_size of rnn\")\n",
    "tf.flags.DEFINE_integer(\"max_len\",500,\"the max number of words in a review\")\n",
    "tf.flags.DEFINE_float(\"lr\", 0.005, \"Learning rate\")\n",
    "tf.flags.DEFINE_float(\"rr\",0.01,'regulariztion rate')\n",
    "tf.flags.DEFINE_integer('attention_dim',100,'Dimensionality of attention layer')\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 100, \"Batch Size\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 1000, \"Number of training epochs\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 25, \"Evaluate model on dev set after this many steps\")\n",
    "\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "ATTENTION_DIM=100\n",
      "BATCH_SIZE=100\n",
      "EMBEDDING_DIM=200\n",
      "EVALUATE_EVERY=25\n",
      "HIDDEN_SIZE=50\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "LR=0.005\n",
      "MAX_LEN=500\n",
      "N_CLASS=3\n",
      "NUM_EPOCHS=1000\n",
      "RR=0.01\n",
      "\n",
      "Loading data...\n",
      "Loading data finished...\n",
      "WARNING:tensorflow:From <ipython-input-2-6d04eda3412f>:61: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "2019-03-08T11:48:21.314076: step 1, loss 1.09321, acc 0.6\n",
      "2019-03-08T11:48:22.224203: step 2, loss 1.04977, acc 0.63\n",
      "2019-03-08T11:48:23.068660: step 3, loss 1.02903, acc 0.52\n",
      "2019-03-08T11:48:23.882710: step 4, loss 1.0286, acc 0.52\n",
      "2019-03-08T11:48:24.717156: step 5, loss 0.84269, acc 0.68\n",
      "2019-03-08T11:48:25.544335: step 6, loss 1.07668, acc 0.52\n",
      "2019-03-08T11:48:26.334584: step 7, loss 0.867726, acc 0.65\n",
      "2019-03-08T11:48:27.107432: step 8, loss 0.957349, acc 0.58\n",
      "2019-03-08T11:48:27.884750: step 9, loss 0.845242, acc 0.7\n",
      "2019-03-08T11:48:28.658709: step 10, loss 1.00922, acc 0.55\n",
      "2019-03-08T11:48:29.426324: step 11, loss 0.966212, acc 0.58\n",
      "2019-03-08T11:48:30.185290: step 12, loss 0.955474, acc 0.59\n",
      "2019-03-08T11:48:30.953387: step 13, loss 0.891498, acc 0.65\n",
      "2019-03-08T11:48:31.733108: step 14, loss 0.9941, acc 0.54\n",
      "2019-03-08T11:48:32.500475: step 15, loss 0.954601, acc 0.58\n",
      "2019-03-08T11:48:33.277574: step 16, loss 0.898497, acc 0.63\n",
      "2019-03-08T11:48:34.041903: step 17, loss 0.880502, acc 0.64\n",
      "2019-03-08T11:48:34.811741: step 18, loss 0.954121, acc 0.57\n",
      "2019-03-08T11:48:35.583479: step 19, loss 0.923615, acc 0.58\n",
      "2019-03-08T11:48:36.336958: step 20, loss 0.941397, acc 0.53\n",
      "2019-03-08T11:48:37.113101: step 21, loss 0.87393, acc 0.6\n",
      "2019-03-08T11:48:37.878125: step 22, loss 0.774533, acc 0.69\n",
      "2019-03-08T11:48:38.631987: step 23, loss 0.918856, acc 0.57\n",
      "2019-03-08T11:48:39.370958: step 24, loss 0.790448, acc 0.65\n",
      "2019-03-08T11:48:40.148818: step 25, loss 0.983399, acc 0.53\n",
      "\n",
      "Evaluation round 1:\n",
      "dev_acc: 0.5019    dev_RMSE: 1.0612\n",
      "test_acc: 0.4433    test_RMSE: 1.1397\n",
      "Saved model checkpoint to /Users/tangze/Desktop/checkpoints/1552016897/model-25\n",
      "\n",
      "topacc: 0.4433   RMSE: 1.1397\n",
      "2019-03-08T11:49:00.997435: step 26, loss 1.01148, acc 0.49\n",
      "2019-03-08T11:49:01.833481: step 27, loss 0.84401, acc 0.62\n",
      "2019-03-08T11:49:02.631989: step 28, loss 0.834185, acc 0.59\n",
      "2019-03-08T11:49:03.427591: step 29, loss 0.888732, acc 0.61\n",
      "2019-03-08T11:49:04.217583: step 30, loss 0.808054, acc 0.62\n",
      "2019-03-08T11:49:04.973813: step 31, loss 0.827574, acc 0.57\n",
      "2019-03-08T11:49:05.760070: step 32, loss 0.87742, acc 0.62\n",
      "2019-03-08T11:49:06.539163: step 33, loss 0.804372, acc 0.63\n",
      "2019-03-08T11:49:07.306369: step 34, loss 0.912515, acc 0.55\n",
      "2019-03-08T11:49:08.104281: step 35, loss 0.913242, acc 0.54\n",
      "2019-03-08T11:49:08.865583: step 36, loss 0.79849, acc 0.62\n",
      "2019-03-08T11:49:09.640625: step 37, loss 0.787974, acc 0.64\n",
      "2019-03-08T11:49:10.399738: step 38, loss 0.788616, acc 0.69\n",
      "2019-03-08T11:49:11.172403: step 39, loss 0.848139, acc 0.67\n",
      "2019-03-08T11:49:11.930665: step 40, loss 0.840652, acc 0.62\n",
      "2019-03-08T11:49:12.742618: step 41, loss 0.755967, acc 0.66\n",
      "2019-03-08T11:49:13.510427: step 42, loss 0.833934, acc 0.64\n",
      "2019-03-08T11:49:14.307509: step 43, loss 0.95003, acc 0.58\n",
      "2019-03-08T11:49:15.036680: step 44, loss 0.832074, acc 0.57\n",
      "2019-03-08T11:49:15.768140: step 45, loss 0.801702, acc 0.6\n",
      "2019-03-08T11:49:16.514344: step 46, loss 0.859468, acc 0.64\n",
      "2019-03-08T11:49:17.262647: step 47, loss 0.712447, acc 0.68\n",
      "2019-03-08T11:49:17.999471: step 48, loss 0.608233, acc 0.74\n",
      "2019-03-08T11:49:18.758578: step 49, loss 0.862552, acc 0.59\n",
      "2019-03-08T11:49:19.492507: step 50, loss 0.75738, acc 0.68\n",
      "\n",
      "Evaluation round 2:\n",
      "dev_acc: 0.5852    dev_RMSE: 0.8905\n",
      "test_acc: 0.5687    test_RMSE: 0.8931\n",
      "Saved model checkpoint to /Users/tangze/Desktop/checkpoints/1552016897/model-50\n",
      "\n",
      "topacc: 0.5687   RMSE: 0.8931\n",
      "2019-03-08T11:49:39.875495: step 51, loss 0.702764, acc 0.71\n",
      "2019-03-08T11:49:40.642036: step 52, loss 0.711341, acc 0.71\n",
      "2019-03-08T11:49:41.426881: step 53, loss 0.811974, acc 0.61\n",
      "2019-03-08T11:49:42.219121: step 54, loss 0.733339, acc 0.69\n",
      "2019-03-08T11:49:43.011880: step 55, loss 0.784069, acc 0.65\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ea000e2cb05a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# Training loop. For each batch...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtr_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainbatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m             \u001b[0mcurrent_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcurrent_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'evaluate_every'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-ea000e2cb05a>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     82\u001b[0m             _, step, loss, accuracy = sess.run(\n\u001b[1;32m     83\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhuapa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhuapa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                 feed_dict)\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0mtime_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misoformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}: step {}, loss {:g}, acc {:g}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS.flag_values_dict()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value.default))\n",
    "print(\"\")\n",
    "FLAGS = FLAGS.__flags\n",
    "\n",
    "#Load data\n",
    "print('Loading data...')\n",
    "trainset = Dataset('./Video_text_train.txt','./Video_simple_feature_train.txt')\n",
    "devset = Dataset('./Video_text_validate.txt','./Video_simple_feature_validate.txt')\n",
    "testset = Dataset('./Video_text_test.txt','./Video_simple_feature_test.txt')\n",
    "\n",
    "embeddingpath = './yelp-2014-embedding-200d.txt'\n",
    "alldata = np.concatenate([trainset.t_docs, devset.t_docs, testset.t_docs], axis=0)\n",
    "embeddingfile, wordsdict = load_embedding(embeddingpath,alldata, FLAGS['embedding_dim'].default)\n",
    "del alldata\n",
    "print(\"Loading data finished...\")\n",
    "\n",
    "#存储字典映射\n",
    "with open(\"./wordsdict.txt\", 'wb') as f:\n",
    "    pickle.dump(wordsdict, f, 0)\n",
    "#存储embeddingfile\n",
    "with open(\"./embeddingfile.txt\",'wb') as f:\n",
    "    pickle.dump(embeddingfile,f,0)\n",
    "\n",
    "trainbatches = trainset.batch_iter(wordsdict, FLAGS['n_class'].default, FLAGS['batch_size'].default,\n",
    "                                 FLAGS['num_epochs'].default, FLAGS['max_len'].default)\n",
    "devset.genBatch(wordsdict,FLAGS['batch_size'].default,FLAGS['max_len'].default,FLAGS['n_class'].default)\n",
    "testset.genBatch(wordsdict,FLAGS['batch_size'].default,FLAGS['max_len'].default,FLAGS['n_class'].default)\n",
    "\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    session_config = tf.ConfigProto(\n",
    "        allow_soft_placement=FLAGS['allow_soft_placement'].default,\n",
    "        log_device_placement=FLAGS['log_device_placement'].default\n",
    "    )\n",
    "    session_config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=session_config)\n",
    "    with sess.as_default():\n",
    "        huapa = HUAPA_ONE_LAYER(\n",
    "            max_len = FLAGS['max_len'].default,\n",
    "            class_num = FLAGS['n_class'].default,\n",
    "            embedding_file = embeddingfile,\n",
    "            embedding_dim = FLAGS['embedding_dim'].default,\n",
    "            hidden_size = FLAGS['hidden_size'].default,\n",
    "            feature_dim = 10,\n",
    "            attention_dim = FLAGS['attention_dim'].default,\n",
    "            train = True,\n",
    "            regularizer = tf.contrib.layers.l2_regularizer(FLAGS['rr'].default)\n",
    "        )\n",
    "        huapa.build_model()\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(FLAGS['lr'].default)\n",
    "        grads_and_vars = optimizer.compute_gradients(huapa.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "        # Save dict\n",
    "        timestamp = str(int(time.time()))\n",
    "        checkpoint_dir = os.path.abspath(\"./checkpoints/\"+timestamp)\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=1)\n",
    "        #with open(checkpoint_dir + \"/wordsdict.txt\", 'wb') as f:\n",
    "        #    pickle.dump(wordsdict, f)\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        def train_step(batch):\n",
    "            features, b_docs, label, doc_len = zip(*batch)\n",
    "            #设置为训练\n",
    "            huapa.train = True\n",
    "            #print(np.array(features).shape,np.array(b_docs).shape,np.array(label).shape,np.array(doc_len).shape)\n",
    "            feed_dict = {\n",
    "                huapa.doc_feature: features,\n",
    "                huapa.input_x: b_docs,\n",
    "                huapa.input_y: label,\n",
    "                huapa.doc_len: doc_len\n",
    "            }\n",
    "            _, step, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, huapa.loss, huapa.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "        \n",
    "        def predict_step(features,b_docs,label,doc_len, name=None):\n",
    "            feed_dict = {\n",
    "                huapa.doc_feature: features,\n",
    "                huapa.input_x: b_docs,\n",
    "                huapa.input_y: label,\n",
    "                huapa.doc_len: doc_len\n",
    "            }\n",
    "            step, loss, accuracy, correct_num, mse = sess.run(\n",
    "                [global_step, huapa.loss, huapa.accuracy, huapa.correct_num, huapa.mse],\n",
    "                feed_dict)\n",
    "            return correct_num, accuracy, mse\n",
    "        \n",
    "        def predict(dataset, name=None):\n",
    "            huapa.train = False\n",
    "            acc = 0\n",
    "            rmse = 0.\n",
    "            for i in range(dataset.epoch):\n",
    "                correct_num, _, mse = predict_step(dataset.features[i], dataset.docs[i], dataset.labels[i],\n",
    "                                                   dataset.doc_len[i], name)\n",
    "                acc += correct_num\n",
    "                rmse += mse\n",
    "            acc = acc * 1.0 / dataset.data_size\n",
    "            rmse = np.sqrt(rmse / dataset.data_size)\n",
    "            return acc, rmse\n",
    "\n",
    "        topacc = 0.\n",
    "        toprmse = 0.\n",
    "        better_dev_acc = 0.\n",
    "        predict_round = 0\n",
    "        \n",
    "        # Training loop. For each batch...\n",
    "        for tr_batch in trainbatches:\n",
    "            train_step(tr_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % FLAGS['evaluate_every'].default == 0:\n",
    "                predict_round += 1\n",
    "                print(\"\\nEvaluation round %d:\" % (predict_round))\n",
    "\n",
    "                dev_acc, dev_rmse = predict(devset, name=\"dev\")\n",
    "                print(\"dev_acc: %.4f    dev_RMSE: %.4f\" % (dev_acc, dev_rmse))\n",
    "                test_acc, test_rmse = predict(testset, name=\"test\")\n",
    "                print(\"test_acc: %.4f    test_RMSE: %.4f\" % (test_acc, test_rmse))\n",
    "\n",
    "                # print topacc with best dev acc\n",
    "                if dev_acc >= better_dev_acc:\n",
    "                    better_dev_acc = dev_acc\n",
    "                    topacc = test_acc\n",
    "                    toprmse = test_rmse\n",
    "                    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                    print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "                print(\"topacc: %.4f   RMSE: %.4f\" % (topacc, toprmse))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成向量表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "ATTENTION_DIM=100\n",
      "BATCH_SIZE=100\n",
      "EMBEDDING_DIM=200\n",
      "EVALUATE_EVERY=25\n",
      "HIDDEN_SIZE=50\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "LR=0.005\n",
      "MAX_LEN=500\n",
      "N_CLASS=3\n",
      "NUM_EPOCHS=1000\n",
      "RR=0.01\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from ../checkpoints/1552022238/model-650\n",
      "\n",
      "test_acc: 0.7172    test_RMSE: 0.6300\n",
      "\n",
      "0.7171645\n",
      "(15433, 100)\n"
     ]
    }
   ],
   "source": [
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS.flag_values_dict()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value.default))\n",
    "print(\"\")\n",
    "FLAGS = FLAGS.__flags\n",
    "\n",
    "checkpoint_file = './checkpoints/1552022238/model-650'\n",
    "#加载数据\n",
    "testset = Dataset('./Video_text.txt','./Video_simple_feature.txt')\n",
    "with open('./wordsdict.txt','rb') as f:\n",
    "    wordsdict = pickle.load(f)\n",
    "testset.getAllData(wordsdict,FLAGS['max_len'].default,FLAGS['n_class'].default)\n",
    "    \n",
    "def evaluate():\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        session_config = tf.ConfigProto(\n",
    "            allow_soft_placement=FLAGS['allow_soft_placement'].default,\n",
    "            log_device_placement=FLAGS['log_device_placement'].default\n",
    "        )\n",
    "        session_config.gpu_options.allow_growth = True\n",
    "        sess = tf.Session(config=session_config)\n",
    "        with sess.as_default():\n",
    "            saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
    "            saver.restore(sess, checkpoint_file)\n",
    "            \n",
    "            input_x = graph.get_operation_by_name(\"input/input_x\").outputs[0]\n",
    "            input_y = graph.get_operation_by_name(\"input/input_y\").outputs[0]\n",
    "            doc_len = graph.get_operation_by_name(\"input/doc_len\").outputs[0]\n",
    "            doc_feature = graph.get_operation_by_name(\"input/doc_feature\").outputs[0]\n",
    "        \n",
    "            hupa_doc_vectors = graph.get_operation_by_name(\"softmax/doc_vectors\").outputs[0]\n",
    "            hupa_correct_num = graph.get_operation_by_name(\"accuracy/correct_num\").outputs[0]\n",
    "            hupa_accuracy = graph.get_operation_by_name(\"accuracy/accuracy\").outputs[0]\n",
    "            hupa_mse = graph.get_operation_by_name(\"metrics/mse\").outputs[0]\n",
    "            \n",
    "            validate_feed = {\n",
    "                input_x : testset.all_docs,\n",
    "                input_y: testset.all_labels,\n",
    "                doc_len : testset.all_docs_len,\n",
    "                doc_feature : testset.all_features\n",
    "            }\n",
    "            doc_vectors,accuracy,correct_num,mse = sess.run([hupa_doc_vectors,hupa_accuracy,hupa_correct_num, hupa_mse],validate_feed)\n",
    "            acc = correct_num * 1.0 / testset.data_size\n",
    "            rmse = np.sqrt(mse / testset.data_size)\n",
    "            \n",
    "            print(\"\\ntest_acc: %.4f    test_RMSE: %.4f\\n\" % (acc, rmse))\n",
    "            print(accuracy)\n",
    "            print(doc_vectors.shape)\n",
    "            np.savetxt('doc_vectors.txt',doc_vectors)\n",
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc: 0.6355    test_RMSE: 0.7299"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1028, 0.4757, 0.1334],\n",
       "        [0.9818, 0.5819, 0.2551],\n",
       "        [0.6113, 0.6436, 0.4179]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(3,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1028, 0.4757, 0.1334],\n",
       "        [0.9818, 0.5819, 0.2551]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.tensor([[0],[1]])\n",
    "b = b.reshape(2)\n",
    "c = torch.tensor([[1,0],[0,1]])\n",
    "d = torch.index_select(a,0,b)\n",
    "e = torch.index_select(d,1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.mean([1,2,3])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 2., 0.],\n",
       "        [3., 2., 0., 0.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.Tensor([[1,0,2,0],[3,2,0,0]])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = np.flatnonzero(a.cpu().data.numpy())\n",
    "d = len(c)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 2., 0.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
